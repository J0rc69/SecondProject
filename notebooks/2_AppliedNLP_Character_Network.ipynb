{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9387167c",
   "metadata": {},
   "source": [
    "# Character Network Analysis\n",
    "\n",
    "This notebook builds character co-occurrence networks for two books (War and Peace and Anna Karenina). It includes:\n",
    "\n",
    "- robust data loading from a `data/` folder\n",
    "- paragraph-level co-occurrence extraction using spaCy NER\n",
    "- conservative name normalization with an **optional manual CSV correction step**\n",
    "- network construction (NetworkX)\n",
    "- network metrics (degree, betweenness) and export of results\n",
    "- side-by-side comparison of the two books (metrics + visualizations)\n",
    "\n",
    "**Usage:** place your book `.txt` files in a `data/` folder next to this notebook, then run cells top to bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea0dcd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 19\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;66;03m# attempt to download (may require internet)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msubprocess\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# 1) Imports & setup\n",
    "from pathlib import Path\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try pyvis for optional interactive export\n",
    "try:\n",
    "    from pyvis.network import Network\n",
    "    HAVE_PYVIS = True\n",
    "except Exception:\n",
    "    HAVE_PYVIS = False\n",
    "\n",
    "# spaCy model load (if not installed, notebook will attempt download when run locally)\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except Exception:\n",
    "    import spacy\n",
    "    try:\n",
    "        # attempt to download (may require internet)\n",
    "        import subprocess, sys\n",
    "        subprocess.run([sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'], check=False)\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "    except Exception:\n",
    "        print('Warning: spaCy model en_core_web_sm not available. NER will fail without it.')\n",
    "        nlp = None\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (10, 6)})\n",
    "\n",
    "# Folders (robust search for data dir)\n",
    "nb_cwd = Path.cwd()\n",
    "project_root = nb_cwd\n",
    "for _ in range(5):\n",
    "    if (project_root / 'data').exists():\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "\n",
    "DATA_DIR = project_root / 'data'\n",
    "OUTDIR = project_root / 'results'\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Project root:', project_root)\n",
    "print('Data directory:', DATA_DIR)\n",
    "print('Results directory:', OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4696c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Helpers: load, clean, split paragraphs\n",
    "\n",
    "def read_text(path: Path) -> str:\n",
    "    return path.read_text(encoding='utf-8', errors='ignore')\n",
    "\n",
    "def strip_gutenberg_headers(text: str) -> str:\n",
    "    start_patterns = [r\"\\*\\*\\* START OF .*?\\*\\*\\*\", r\"START OF THIS PROJECT GUTENBERG EBOOK\"]\n",
    "    end_patterns = [r\"\\*\\*\\* END OF .*?\\*\\*\\*\", r\"END OF THIS PROJECT GUTENBERG EBOOK\"]\n",
    "    start_idx = 0\n",
    "    for pat in start_patterns:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if m:\n",
    "            start_idx = m.end()\n",
    "            break\n",
    "    end_idx = len(text)\n",
    "    for pat in end_patterns:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE | re.DOTALL)\n",
    "        if m:\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "    return text[start_idx:end_idx].strip()\n",
    "\n",
    "def normalize_whitespace(text: str) -> str:\n",
    "    text = text.replace('\\r\\n', '\\n')\n",
    "    text = re.sub(r\"[ \\t]+\", ' ', text)\n",
    "    text = re.sub(r\"\\n{3,}\", '\\n\\n', text)\n",
    "    return text\n",
    "\n",
    "def split_paragraphs(text: str):\n",
    "    text = normalize_whitespace(text)\n",
    "    parts = [p.strip() for p in re.split(r\"\\n\\n+\", text) if p.strip()]\n",
    "    return parts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8680e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) NER extraction (paragraph-level)\n",
    "\n",
    "def extract_persons_from_paragraph(paragraph: str):\n",
    "    \"\"\"Return raw PERSON entity strings from the paragraph using spaCy (if available).\"\"\"\n",
    "    if nlp is None:\n",
    "        # fallback naive extraction: capitalized word sequences (risky but keeps notebook runnable)\n",
    "        candidates = re.findall(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b\", paragraph)\n",
    "        return candidates\n",
    "    doc = nlp(paragraph)\n",
    "    return [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c702302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Name cleaning and automatic mapping\n",
    "\n",
    "def clean_name(name: str) -> str:\n",
    "    name = name.strip()\n",
    "    name = name.strip('\\\"\\'`\\n\\t.,;:()[]')\n",
    "    name = re.sub(r\"\\s+\", ' ', name)\n",
    "    return name\n",
    "\n",
    "def build_name_mapping(person_mentions: Counter, min_freq_for_fullname=3):\n",
    "    cleaned_counts = Counter()\n",
    "    for name, cnt in person_mentions.items():\n",
    "        c = clean_name(name)\n",
    "        if c:\n",
    "            cleaned_counts[c] += cnt\n",
    "\n",
    "    fullnames = {n: c for n, c in cleaned_counts.items() if len(n.split()) >= 2 and c >= min_freq_for_fullname}\n",
    "\n",
    "    mapping = {}\n",
    "    for name in cleaned_counts:\n",
    "        mapping[name] = name\n",
    "\n",
    "    for name, cnt in cleaned_counts.items():\n",
    "        if len(name.split()) == 1:\n",
    "            first = name\n",
    "            candidates = [(fn, fullnames[fn]) for fn in fullnames if fn.split()[0] == first]\n",
    "            if len(candidates) == 1:\n",
    "                mapping[name] = candidates[0][0]\n",
    "            elif len(candidates) > 1:\n",
    "                candidates_sorted = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "                mapping[name] = candidates_sorted[0][0]\n",
    "    return mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cca4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Co-occurrence counting and manual mapping support\n",
    "\n",
    "def count_cooccurrences(paragraphs):\n",
    "    raw_person_counter = Counter()\n",
    "    paragraphs_persons = []\n",
    "    for p in paragraphs:\n",
    "        persons = extract_persons_from_paragraph(p)\n",
    "        persons = [clean_name(x) for x in persons if clean_name(x)]\n",
    "        for name in persons:\n",
    "            raw_person_counter[name] += 1\n",
    "        paragraphs_persons.append(persons)\n",
    "\n",
    "    auto_mapping = build_name_mapping(raw_person_counter)\n",
    "\n",
    "    # Write a template manual mapping CSV if not present, containing top names\n",
    "    manual_csv = OUTDIR / 'manual_name_mapping.csv'\n",
    "    if not manual_csv.exists():\n",
    "        df_template = pd.DataFrame([{'raw_name': k, 'canonical_name': auto_mapping.get(k, k)} for k in raw_person_counter.most_common(200)])\n",
    "        df_template.to_csv(manual_csv, index=False)\n",
    "        print(f\"Wrote manual mapping template to: {manual_csv} -- edit this file and re-run 'apply_manual_mapping' cell to correct names.\")\n",
    "    else:\n",
    "        print(f\"Manual mapping CSV found: {manual_csv}. Will be used if you call apply_manual_mapping().\")\n",
    "\n",
    "    # default mapping to use now (auto)\n",
    "    mapping_in_use = auto_mapping.copy()\n",
    "\n",
    "    # Apply mapping and compute counts\n",
    "    person_counts = Counter()\n",
    "    cooc = Counter()\n",
    "    for persons in paragraphs_persons:\n",
    "        canonical = [mapping_in_use.get(p, p) for p in persons]\n",
    "        uniq = set(canonical)\n",
    "        for p in uniq:\n",
    "            person_counts[p] += 1\n",
    "        for a, b in itertools.combinations(sorted(uniq), 2):\n",
    "            cooc[frozenset((a, b))] += 1\n",
    "\n",
    "    return raw_person_counter, mapping_in_use, person_counts, cooc\n",
    "\n",
    "def apply_manual_mapping(raw_person_counter, paragraphs):\n",
    "    \"\"\"Load manual mapping CSV (if exists) and apply it, returning (mapping, person_counts, cooc).\n",
    "    Use after editing the manual CSV file produced by count_cooccurrences.\n",
    "    \"\"\"\n",
    "    manual_csv = OUTDIR / 'manual_name_mapping.csv'\n",
    "    if not manual_csv.exists():\n",
    "        raise FileNotFoundError(f\"Manual mapping CSV not found at {manual_csv}. Run the earlier cell to generate a template.\")\n",
    "    df = pd.read_csv(manual_csv)\n",
    "    manual_map = {row['raw_name']: row['canonical_name'] for _, row in df.dropna(subset=['raw_name']).iterrows()}\n",
    "\n",
    "    # Re-extract persons from paragraphs and apply mapping\n",
    "    paragraphs_persons = []\n",
    "    for p in paragraphs:\n",
    "        persons = extract_persons_from_paragraph(p)\n",
    "        persons = [clean_name(x) for x in persons if clean_name(x)]\n",
    "        paragraphs_persons.append(persons)\n",
    "\n",
    "    person_counts = Counter()\n",
    "    cooc = Counter()\n",
    "    for persons in paragraphs_persons:\n",
    "        canonical = [manual_map.get(p, p) for p in persons]\n",
    "        uniq = set(canonical)\n",
    "        for p in uniq:\n",
    "            person_counts[p] += 1\n",
    "        for a, b in itertools.combinations(sorted(uniq), 2):\n",
    "            cooc[frozenset((a, b))] += 1\n",
    "\n",
    "    return manual_map, person_counts, cooc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e84ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Build NetworkX graph, compute metrics, and export results\n",
    "\n",
    "def build_graph(person_counts: Counter, cooc: Counter, min_edge_weight=1):\n",
    "    G = nx.Graph()\n",
    "    for name, cnt in person_counts.items():\n",
    "        G.add_node(name, count=cnt)\n",
    "    for pair_fs, w in cooc.items():\n",
    "        if w >= min_edge_weight:\n",
    "            a, b = tuple(pair_fs)\n",
    "            G.add_edge(a, b, weight=w)\n",
    "    return G\n",
    "\n",
    "def compute_and_export_metrics(G: nx.Graph, book_tag: str, top_n=50):\n",
    "    # Degree (count of edges) and weighted degree\n",
    "    deg = dict(G.degree())\n",
    "    weighted_deg = dict(G.degree(weight='weight'))\n",
    "    # Centralities\n",
    "    try:\n",
    "        bet = nx.betweenness_centrality(G, weight='weight')\n",
    "    except Exception:\n",
    "        bet = nx.betweenness_centrality(G)\n",
    "\n",
    "    # Assemble dataframe\n",
    "    df = pd.DataFrame([{\n",
    "        'character': n,\n",
    "        'degree': deg.get(n, 0),\n",
    "        'weighted_degree': weighted_deg.get(n, 0),\n",
    "        'betweenness': bet.get(n, 0),\n",
    "        'mentions': G.nodes[n].get('count', 0)\n",
    "    } for n in G.nodes()])\n",
    "\n",
    "    df = df.sort_values(by='weighted_degree', ascending=False)\n",
    "    out_csv = OUTDIR / f\"{book_tag.replace(' ', '_')}_node_metrics.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved node metrics to: {out_csv}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Visualization helpers\n",
    "\n",
    "def draw_networkx(G: nx.Graph, top_n_nodes=40, title=None, savepath=None):\n",
    "    if top_n_nodes and G.number_of_nodes() > top_n_nodes:\n",
    "        nodes_sorted = sorted(G.nodes(data=True), key=lambda x: x[1].get('count', 0), reverse=True)[:top_n_nodes]\n",
    "        top_nodes = {n for n, _ in nodes_sorted}\n",
    "        H = G.subgraph(top_nodes).copy()\n",
    "    else:\n",
    "        H = G\n",
    "\n",
    "    pos = nx.spring_layout(H, k=0.5, seed=42)\n",
    "    node_sizes = [H.nodes[n].get('count', 1) * 40 for n in H.nodes]\n",
    "    edge_widths = [max(0.5, H[u][v].get('weight', 1) / 2) for u, v in H.edges()]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    nx.draw_networkx_nodes(H, pos, node_size=node_sizes)\n",
    "    nx.draw_networkx_edges(H, pos, width=edge_widths, alpha=0.7)\n",
    "    nx.draw_networkx_labels(H, pos, font_size=9)\n",
    "    plt.title(title or 'Character Network')\n",
    "    plt.axis('off')\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def export_pyvis(G: nx.Graph, output_file=None):\n",
    "    if not HAVE_PYVIS:\n",
    "        print('pyvis not available; skipping interactive export')\n",
    "        return None\n",
    "    net = Network(height='750px', width='100%', notebook=False)\n",
    "    for n, d in G.nodes(data=True):\n",
    "        net.add_node(n, label=n, title=f\"{n} (mentions: {d.get('count',0)})\", value=d.get('count',1))\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        net.add_edge(u, v, value=d.get('weight',1), title=f\"w={d.get('weight',1)}\")\n",
    "    if output_file:\n",
    "        net.show(output_file)\n",
    "        print(f\"Wrote interactive HTML to: {output_file}\")\n",
    "        return output_file\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35b4e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Pipeline function to run end-to-end for a single book\n",
    "\n",
    "def run_book(filepath: Path, book_tag: str, top_n_nodes=40, min_edge_weight=1):\n",
    "    print('\\nProcessing', filepath, 'as', book_tag)\n",
    "    raw = read_text(filepath)\n",
    "    cleaned = strip_gutenberg_headers(raw)\n",
    "    paragraphs = split_paragraphs(cleaned)\n",
    "    print('Paragraphs:', len(paragraphs))\n",
    "\n",
    "    raw_person_counter, auto_mapping, person_counts_auto, cooc_auto = count_cooccurrences(paragraphs)\n",
    "\n",
    "    # Build auto graph\n",
    "    G_auto = build_graph(person_counts_auto, cooc_auto, min_edge_weight=min_edge_weight)\n",
    "\n",
    "    # Save auto results\n",
    "    png_auto = OUTDIR / f\"{book_tag.replace(' ','_')}_network_auto.png\"\n",
    "    draw_networkx(G_auto, top_n_nodes=top_n_nodes, title=f\"{book_tag} (auto mapping)\", savepath=png_auto)\n",
    "    metrics_auto = compute_and_export_metrics(G_auto, f\"{book_tag}_auto\")\n",
    "\n",
    "    # At this point user can edit OUTDIR/manual_name_mapping.csv and then call apply_manual_mapping\n",
    "    manual_csv = OUTDIR / 'manual_name_mapping.csv'\n",
    "\n",
    "    return {\n",
    "        'paragraphs': paragraphs,\n",
    "        'raw_person_counter': raw_person_counter,\n",
    "        'auto_mapping': auto_mapping,\n",
    "        'person_counts_auto': person_counts_auto,\n",
    "        'cooc_auto': cooc_auto,\n",
    "        'G_auto': G_auto,\n",
    "        'metrics_auto': metrics_auto,\n",
    "        'manual_csv': manual_csv,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56125db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Run pipeline for both books\n",
    "# Ensure your text files are in the DATA_DIR\n",
    "DATA_DIR = Path(DATA_DIR)\n",
    "war_candidates = list(DATA_DIR.glob('*War*Peace*.txt')) + list(DATA_DIR.glob('*war*peace*.txt'))\n",
    "anna_candidates = list(DATA_DIR.glob('*Anna*Karin*.txt')) + list(DATA_DIR.glob('*Anna*Karenina*.txt'))\n",
    "\n",
    "if not war_candidates:\n",
    "    print('No War and Peace candidate found in', DATA_DIR)\n",
    "if not anna_candidates:\n",
    "    print('No Anna Karenina candidate found in', DATA_DIR)\n",
    "\n",
    "war_file = war_candidates[0] if war_candidates else None\n",
    "anna_file = anna_candidates[0] if anna_candidates else None\n",
    "\n",
    "results = {}\n",
    "if war_file:\n",
    "    results['war'] = run_book(war_file, 'War_and_Peace', top_n_nodes=40)\n",
    "else:\n",
    "    print('Please add War and Peace text to data/')\n",
    "\n",
    "if anna_file:\n",
    "    results['anna'] = run_book(anna_file, 'Anna_Karenina', top_n_nodes=35)\n",
    "else:\n",
    "    print('Please add Anna Karenina text to data/')\n",
    "\n",
    "print('\\nRun complete. If you want to apply manual mapping corrections, edit the file:')\n",
    "print(results.get('war', {}).get('manual_csv') or results.get('anna', {}).get('manual_csv'))\n",
    "print('Then run the \"Apply manual mapping\" cell below to recompute graphs and metrics using your canonical names.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03bbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Apply manual mapping (after you edit results/manual_name_mapping.csv)\n",
    "# This cell will load the manual CSV and rebuild graphs/metrics using the manual mapping\n",
    "\n",
    "def apply_manual_for_all(results_dict):\n",
    "    recomputed = {}\n",
    "    for key, res in results_dict.items():\n",
    "        print('\\nApplying manual mapping for', key)\n",
    "        paragraphs = res['paragraphs']\n",
    "        manual_map, person_counts_manual, cooc_manual = apply_manual_mapping(res['raw_person_counter'], paragraphs)\n",
    "        G_manual = build_graph(person_counts_manual, cooc_manual, min_edge_weight=1)\n",
    "        png_manual = OUTDIR / f\"{key}_network_manual.png\"\n",
    "        draw_networkx(G_manual, top_n_nodes=40, title=f\"{key} (manual mapping)\", savepath=png_manual)\n",
    "        metrics_manual = compute_and_export_metrics(G_manual, f\"{key}_manual\")\n",
    "        recomputed[key] = {\n",
    "            'manual_map': manual_map,\n",
    "            'G_manual': G_manual,\n",
    "            'metrics_manual': metrics_manual,\n",
    "            'png_manual': png_manual\n",
    "        }\n",
    "    return recomputed\n",
    "\n",
    "# To run this cell, simply execute: recomputed = apply_manual_for_all(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Compare the two networks side-by-side (metrics + visualization)\n",
    "# This cell expects 'results' (from the run cell) and optionally 'recomputed' (after manual mapping)\n",
    "\n",
    "comparison_df = []\n",
    "\n",
    "# Prefer manual metrics if available\n",
    "use_manual = 'recomputed' in globals()\n",
    "\n",
    "for key in ['war', 'anna']:\n",
    "    if key not in results:\n",
    "        continue\n",
    "    if use_manual and key in recomputed:\n",
    "        metrics = recomputed[key]['metrics_manual']\n",
    "        tag = key + '_manual'\n",
    "    else:\n",
    "        metrics = results[key]['metrics_auto']\n",
    "        tag = key + '_auto'\n",
    "\n",
    "    # top 10 by weighted_degree\n",
    "    top10 = metrics.head(10).copy()\n",
    "    top10['book'] = tag\n",
    "    comparison_df.append(top10)\n",
    "\n",
    "if comparison_df:\n",
    "    comp = pd.concat(comparison_df, ignore_index=True)\n",
    "    # Save combined CSV\n",
    "    comp_csv = OUTDIR / 'comparison_top10_metrics.csv'\n",
    "    comp.to_csv(comp_csv, index=False)\n",
    "    print('Saved comparison table to', comp_csv)\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(comp)\n",
    "    except Exception:\n",
    "        print(comp)\n",
    "\n",
    "# Visualizations: show the two PNGs (auto or manual) sequentially\n",
    "for key in ['war', 'anna']:\n",
    "    if use_manual and key in recomputed:\n",
    "        path = recomputed[key]['png_manual']\n",
    "    else:\n",
    "        path = OUTDIR / f\"{key}_network_auto.png\"\n",
    "    print('\\nImage for', key, ':', path)\n",
    "    \n",
    "    try:\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(str(path)))\n",
    "    except Exception:\n",
    "        print('Cannot display image inline; find it at', path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857c4fe8",
   "metadata": {},
   "source": [
    "## Final notes\n",
    "\n",
    "- After running the notebook once, a template manual mapping CSV will be written to `results/manual_name_mapping.csv` containing the most frequent raw extracted names. Edit that CSV to supply `canonical_name` values for rows where you'd like to merge variants (for example mapping `Pierre` -> `Pierre Bezukhov`).\n",
    "- Once edited, run the **Apply manual mapping** cell to rebuild the graphs and export updated metrics/PNGs.\n",
    "- The notebook exports CSVs for node metrics and a combined comparison table in the `results/` folder for inclusion in your write-up.\n",
    "\n",
   ]
  }
 
