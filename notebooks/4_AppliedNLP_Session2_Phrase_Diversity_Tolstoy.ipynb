{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-intro",
   "metadata": {},
   "source": [
    "# 4) Phrase Diversity (n-gram TTR)\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "## Overview\n",
    "- Measure phrase diversity using **Type–Token Ratio (TTR)** for **bigrams** and **trigrams**.\n",
    "- Compare lexical variety across two works by the same author (Leo Tolstoy).\n",
    "- (Optional) Analyze diversity trends across sections/chapters within the texts.\n",
    "\n",
    "## Learning objectives\n",
    "- Understand and compute TTR as a measure of lexical/phrasal diversity.\n",
    "- Apply TTR to n-grams (bigrams, trigrams) to quantify phrase variety.\n",
    "- Interpret TTR values: lower TTR suggests formulaic/repetitive phrasing, higher TTR indicates more varied expression.\n",
    "- Produce reproducible diversity metrics and visualizations for literary analysis.\n",
    "\n",
    "## Quick start\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books (already set to Tolstoy).\n",
    "2. (Optional) Toggle `use_stopwords` to exclude common function words.\n",
    "3. Run cells from top to bottom. The main outputs are saved to `results/`.\n",
    "4. (Optional) Customize section-splitting regex to analyze diversity by chapter.\n",
    "\n",
    "## Prerequisites\n",
    "- A Python environment with packages: `pandas`, `matplotlib`.\n",
    "- The text files for the two works placed in `data/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "Fill the `CONFIG` paths for your two books (plain text). Toggle stopwords as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"data/War-and-Peace.txt\",     # <-- Tolstoy, English translation\n",
    "    \"book2_path\": \"data/Anna-Karenina.txt\",     # <-- Tolstoy, English translation\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,           # toggle\n",
    "    \"min_ngram_count\": 5,             # threshold (unused here but kept for parity)\n",
    "    \"top_k\": 20                       # top items to show (unused here but kept for parity)\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-normalize-header",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are from Project Gutenberg; we strip the boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stripper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    t = text.replace(\"\\ufeff\", \"\")\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-norm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    t = strip_gutenberg(t)\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "len(tokens1), len(tokens2), len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compute-ttr-header",
   "metadata": {},
   "source": [
    "## 2. Compute TTR for Bigrams & Trigrams\n",
    "TTR for n-grams measures phrase variety: `unique n-grams / total n-grams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-ttr",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(seq):\n",
    "    return len(set(seq)) / max(1, len(seq))\n",
    "\n",
    "bigrams_list = list(zip(tokens, tokens[1:]))\n",
    "trigrams_list = list(zip(tokens, tokens[1:], tokens[2:]))\n",
    "\n",
    "ttr2_combined = ttr(bigrams_list)\n",
    "ttr3_combined = ttr(trigrams_list)\n",
    "\n",
    "bigrams_list_1 = list(zip(tokens1, tokens1[1:]))\n",
    "trigrams_list_1 = list(zip(tokens1, tokens1[1:], tokens1[2:]))\n",
    "ttr2_book1 = ttr(bigrams_list_1)\n",
    "ttr3_book1 = ttr(trigrams_list_1)\n",
    "\n",
    "bigrams_list_2 = list(zip(tokens2, tokens2[1:]))\n",
    "trigrams_list_2 = list(zip(tokens2, tokens2[1:], tokens2[2:]))\n",
    "ttr2_book2 = ttr(bigrams_list_2)\n",
    "ttr3_book2 = ttr(trigrams_list_2)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"metric\": [\"bigram_TTR\", \"trigram_TTR\"],\n",
    "    \"combined\": [ttr2_combined, ttr3_combined],\n",
    "    \"book1\": [ttr2_book1, ttr3_book1],\n",
    "    \"book2\": [ttr2_book2, ttr3_book2]\n",
    "})\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## 2.1 Visualize TTR Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig_ttr, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(summary_df[\"metric\"]))\n",
    "width = 0.25\n",
    "bars1 = ax.bar(x - width, summary_df[\"book1\"], width, label='Book 1')\n",
    "bars2 = ax.bar(x,           summary_df[\"book2\"], width, label='Book 2')\n",
    "bars3 = ax.bar(x + width,   summary_df[\"combined\"], width, label='Combined')\n",
    "ax.set_ylabel(\"TTR Score\", fontsize=12)\n",
    "ax.set_xlabel(\"N-gram Type\", fontsize=12)\n",
    "ax.set_title(\"Phrase Diversity Comparison: Type-Token Ratio\", fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary_df[\"metric\"])\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(0, float(summary_df[[\"book1\", \"book2\", \"combined\"]].max().max()) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bars in (bars1, bars2, bars3):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-section",
   "metadata": {},
   "source": [
    "## 3. (Optional) Section-wise Diversity\n",
    "If chapters/sections are detectable by regex, estimate diversity per section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scaffold (disabled by default):\n",
    "# full_text = text1  # or text2\n",
    "# sections = re.split(r\"\\bchapter\\b|\\bbook\\b|\\bpart\\b\", full_text, flags=re.IGNORECASE)\n",
    "# rows = []\n",
    "# for i, sec in enumerate(sections, start=1):\n",
    "#     toks = WORD_RE.findall(sec.lower())\n",
    "#     b2 = list(zip(toks, toks[1:]))\n",
    "#     b3 = list(zip(toks, toks[1:], toks[2:]))\n",
    "#     rows.append({\"section\": i, \"bigram_TTR\": ttr(b2), \"trigram_TTR\": ttr(b3)})\n",
    "# sec_df = pd.DataFrame(rows)\n",
    "# ax = sec_df.plot(x=\"section\", y=[\"bigram_TTR\",\"trigram_TTR\"])\n",
    "# ax.set_title(\"Phrase Diversity by Section (proxy)\")\n",
    "# ax.set_xlabel(\"Section index\")\n",
    "# ax.set_ylabel(\"TTR\")\n",
    "# plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "notes-header",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "- **Lower TTR** → more *formulaic/repetitive phrasing*.\n",
    "- **Higher TTR** → more *varied phrasing* / lexical diversity.\n",
    "- Section-wise analysis can reveal diversity shifts across the book.\n",
    "- Removing stopwords may inflate TTR by dropping frequent function-word bigrams (e.g., “of the”, “in the”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflection-header",
   "metadata": {},
   "source": [
    "## 5. Reflection\n",
    "Answer briefly here or in your project README:\n",
    "1. Which results matched your reading intuition?\n",
    "2. What surprised you about the diversity differences?\n",
    "3. Did stopword removal change the overall trend?\n",
    "4. Comparing across the two works: are the patterns stable or different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"results\").mkdir(exist_ok=True)\n",
    "summary_df.to_csv(\"results/TTR_table.csv\", index=False)\n",
    "try:\n",
    "    fig_ttr.savefig(\"results/TTR_figure.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"\\u2713 Saved TTR_table.csv and TTR_figure.png to results/\")\n",
    "except NameError:\n",
    "    print(\"\\u26A0 Figure not saved - run the visualization cell first\")\n",
    "    try:\n",
    "        plt.savefig(\"results/TTR_figure.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
