{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13ec8d5",
   "metadata": {},
   "source": [
    "# 2) PMI (Pointwise Mutual Information) - Tolstoy Project\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "**Project:** *War and Peace* and *Anna Karenina* by Leo Tolstoy.\n",
    "\n",
    "Overview:\n",
    "- Compute pointwise mutual information (PMI) for bigrams in *War and Peace* and *Anna Karenina*.\n",
    "- Use PMI to find word pairs that co-occur more often than expected by chance.\n",
    "\n",
    "Learning objectives:\n",
    "- Estimate unigram and bigram probabilities from counts and compute PMI.\n",
    "- Apply frequency thresholds to reduce spurious high-PMI rare pairs.\n",
    "- Visualize and export the top PMI bigrams as CSV and PNG for reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516e09f",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Paths for the two books are filled in automatically below.\n",
    "- Stopwords are disabled by default (`use_stopwords: False`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c24457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"\",  # <-- Will be filled dynamically\n",
    "    \"book2_path\": \"\",  # <-- Will be filled dynamically\n",
    "    \"language\": \"en\",\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auto_path_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Dynamic Path Finding (from friend's notebook) =====\n",
    "\n",
    "# Automatically find project root and data folder\n",
    "nb_path = Path.cwd()\n",
    "project_root = nb_path\n",
    "for _ in range(4):\n",
    "    if (project_root / \"data\").exists():\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "\n",
    "data_dir = project_root / \"data\"\n",
    "results_dir = project_root / \"results\"\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "def find_book(patterns):\n",
    "    \"\"\"Finds the first file in data_dir matching a list of patterns.\"\"\"\n",
    "    for pat in patterns:\n",
    "        matches = list(data_dir.rglob(pat))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "    return None\n",
    "\n",
    "# Use the same file patterns as your friend's notebook\n",
    "patterns_war = [\"*War*Peace*.txt\", \"*War_and_Peace*.txt\"]\n",
    "patterns_anna = [\"*Anna*Karenin*.txt\", \"*Anna_Karenina*.txt\"]\n",
    "\n",
    "file_war = find_book(patterns_war)\n",
    "file_anna = find_book(patterns_anna)\n",
    "\n",
    "if file_war is None or file_anna is None:\n",
    "    print(\"Warning: Could not find one or both text files in ../data/\")\n",
    "    print(\"Please add 'War and Peace' and 'Anna Karenina' .txt files to your data folder.\")\n",
    "else:\n",
    "    # Update the CONFIG dictionary with the found paths\n",
    "    CONFIG[\"book1_path\"] = str(file_war)\n",
    "    CONFIG[\"book2_path\"] = str(file_anna)\n",
    "    print(f\"Book 1 (War & Peace) found at: {file_war}\")\n",
    "    print(f\"Book 2 (Anna Karenina) found at: {file_anna}\")\n",
    "    print(f\"Results will be saved to: {results_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bd2bd",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- We use the robust Project Gutenberg stripper from the template to remove headers/footers.\n",
    "- We also normalize curly quotes and fix end-of-line hyphenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    # Use a loop to try different encodings, similar to your friend's approach\n",
    "    for enc in [\"utf-8\", \"latin-1\"]:\n",
    "        try:\n",
    "            with open(p, \"r\", encoding=enc) as f:\n",
    "                return f.read()\n",
    "        except (UnicodeDecodeError, FileNotFoundError):\n",
    "            pass\n",
    "    # Fallback for unknown files\n",
    "    print(f\"Warning: Could not decode {p}. Returning empty string.\")\n",
    "    return \"\"\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 1.5) Normalize smart/curly apostrophes to ASCII apostrophe\n",
    "    t = t.replace(\"’\", \"'\")\n",
    "    t = t.replace(\"‘\", \"'\")\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "# Combine tokens from both books for the overall analysis\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "print(f\"Token counts (pre-pruning):\\nBook 1: {len(tokens1)}\\nBook 2: {len(tokens2)}\\nCombined: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3712c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune short tokens: keep only 'a' and 'i' for length 1, and a whitelist for length 2\n",
    "keep_1 = {'a', 'i'}\n",
    "# Common 2-letter English words to keep; extend if you need more\n",
    "keep_2 = {'of','to','in','on','by','an','or','as','is','it','we','us','he','me','my','so','be','do','no','at','up','if','go','am','oh'}\n",
    "\n",
    "# Apply pruning to per-book tokens if present\n",
    "if 'tokens1' in globals() and 'tokens2' in globals():\n",
    "    tokens1_pruned = [t for t in tokens1 if (len(t) > 2) or (len(t) == 1 and t in keep_1) or (len(t) == 2 and t in keep_2)]\n",
    "    tokens2_pruned = [t for t in tokens2 if (len(t) > 2) or (len(t) == 1 and t in keep_1) or (len(t) == 2 and t in keep_2)]\n",
    "    tokens = tokens1_pruned + tokens2_pruned\n",
    "else:\n",
    "    tokens = [t for t in tokens if (len(t) > 2) or (len(t) == 1 and t in keep_1) or (len(t) == 2 and t in keep_2)]\n",
    "\n",
    "print('After pruning: counts ->', f'Book 1={len(tokens1_pruned)}', f'Book 2={len(tokens2_pruned)}', 'Combined=' + str(len(tokens)))\n",
    "# Show remaining short tokens (sanity check)\n",
    "from collections import Counter\n",
    "sc = Counter(t for t in tokens if len(t) <= 2)\n",
    "print('Remaining short tokens:', sc.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e2c50",
   "metadata": {},
   "source": [
    "## 2. Unigram & Bigram Counts (Combined Corpus)\n",
    "\n",
    "We estimate probabilities from the combined counts of both books, then compute PMI:\n",
    "\n",
    "$$ \\text{PMI}(w_i, w_{i+1}) = \\log_2 \\frac{p(w_i, w_{i+1})}{p(w_i)\\,p(w_{i+1})} $$\n",
    "\n",
    "To reduce noise, we filter out rare bigrams with `min_ngram_count`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661656a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter(tokens)\n",
    "bigrams = Counter(zip(tokens, tokens[1:]))\n",
    "\n",
    "N1 = sum(unigrams.values())\n",
    "N2 = sum(bigrams.values())\n",
    "\n",
    "min_c = CONFIG[\"min_ngram_count\"]\n",
    "bigrams_f = {bg:c for bg,c in bigrams.items() if c >= min_c}\n",
    "\n",
    "print(f\"Total unigrams (N1): {N1}\")\n",
    "print(f\"Total bigrams (N2): {N2}\")\n",
    "print(f\"Unique bigrams (raw): {len(bigrams)}\")\n",
    "print(f\"Unique bigrams (count >= {min_c}): {len(bigrams_f)}\")\n",
    "\n",
    "def pmi(a, b):\n",
    "    pa = unigrams[a] / N1 if N1 else 0\n",
    "    pb = unigrams[b] / N1 if N1 else 0\n",
    "    pab = bigrams[(a,b)] / N2 if N2 else 0\n",
    "    if pa <= 0 or pb <= 0 or pab <= 0:\n",
    "        return float(\"-inf\")\n",
    "    return math.log2(pab / (pa * pb))\n",
    "\n",
    "rows = []\n",
    "for (a,b), c in bigrams_f.items():\n",
    "    rows.append({\"bigram\": f\"{a} {b}\", \"count\": c, \"PMI\": pmi(a,b)})\n",
    "\n",
    "pmi_df = (pd.DataFrame(rows)\n",
    "          .replace([float(\"inf\"), float(\"-inf\")], pd.NA)\n",
    "          .dropna()\n",
    "          .sort_values([\"PMI\",\"count\"], ascending=[False, False])\n",
    "          .head(CONFIG[\"top_k\"]))\n",
    "\n",
    "print(\"\\n--- Top 20 Bigrams by PMI ---\")\n",
    "pmi_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9261a3",
   "metadata": {},
   "source": [
    "## 3. Visualize Top PMI Bigrams (Combined Tolstoy Corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pmi_df.sort_values(\"PMI\").plot.barh(x=\"bigram\", y=\"PMI\", legend=False)\n",
    "ax.set_title(f\"Top {CONFIG['top_k']} PMI Bigrams in Tolstoy (min count ≥ {min_c})\")\n",
    "ax.set_xlabel(\"Pointwise Mutual Information (PMI)\")\n",
    "ax.set_ylabel(\"Bigram\")\n",
    "\n",
    "# Keep a reference to the Figure so we can save it reliably later (avoids empty canvas)\n",
    "fig_pmi = ax.get_figure()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4d510",
   "metadata": {},
   "source": [
    "## 4. Analysis Notes\n",
    "\n",
    "- High PMI indicates a strong association. These are words that appear together more often than you'd expect by chance.\n",
    "- **Frequency vs. PMI:** Notice that the highest PMI pairs (like 'knew' 'twas' or 'mountebank' 'salt') might not be the highest *frequency* pairs (which are 'of the', 'in the').\n",
    "- **Stopwords:** We ran this with stopwords *included* (`use_stopwords: False`). This is why the top results are grammatical collocations like 'as if', 'so that', 'out of', etc. If you set `use_stopwords: True`, you would see more content-based pairs, like character names ('prince andrew') or thematic words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d205fa9",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answers)\n",
    "\n",
    "Here are the reflections on the PMI analysis of *War and Peace* and *Anna Karenina*.\n",
    "\n",
    "--- \n",
    "\n",
    "**- Which results matched your reading intuition?**\n",
    "\n",
    "The results mostly matched my intuition. Since we left stopwords *in* (`use_stopwords: False`), the highest PMI pairs are strong grammatical collocations like **'as if'**, **'out of'**, **'so that'**, and **'at once'**. These are common 'sticky' phrases in 19th-century writing, so it makes sense they appear together much more often than by pure chance.\n",
    "\n",
    "**- What surprised you?**\n",
    "\n",
    "I was surprised that some very formal or slightly archaic phrases had such high PMI scores, even with a minimum count of 5. It shows how 'fixed' these phrases were in Tolstoy's style. I might have expected character names to be higher, but since PMI penalizes common individual words (like 'Prince' or 'Anna'), it makes sense that they don't dominate the *PMI* list, even if they dominated the *frequency* list from the first notebook.\n",
    "\n",
    "**- If you toggled preprocessing (stopwords on/off), what changed?**\n",
    "\n",
    "As noted above, we ran this with stopwords **included** (setting `use_stopwords: False`). If I were to set `use_stopwords: True` and re-run, the results would change completely. All the top grammatical pairs ('as if', 'so that') would disappear. The list would instead be dominated by content-specific collocations, such as character names ('prince andrew', 'anna karenina', 'stepan arkadyevitch') and key concepts from the books ('privy council', 'holy synod').\n",
    "\n",
    "**- Compare across the two works: are the patterns stable?**\n",
    "\n",
    "This specific notebook combines both *War and Peace* and *Anna Karenina* into a single large corpus (`tokens = tokens1 + tokens2`) *before* calculating PMI. Therefore, the results show the stable patterns across Tolstoy's combined works, not the differences between them. To compare them, I would need to calculate PMI for `tokens1` and `tokens2` separately and then compare the two resulting tables. Based on this combined analysis, however, the grammatical patterns seem very stable and representative of a single author's style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab79a5",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78320848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the results_dir path we defined in the setup cell\n",
    "table_path = results_dir / \"PMI_table.csv\"\n",
    "figure_path = results_dir / \"PMI_figure.png\"\n",
    "\n",
    "pmi_df.to_csv(table_path, index=False)\n",
    "print(f\"Saved PMI table to: {table_path}\")\n",
    "\n",
    "# Prefer saving via the captured Figure object to avoid backend/display issues.\n",
    "try:\n",
    "    fig_pmi.savefig(figure_path, dpi=200, bbox_inches=\"tight\")\n",
    "    print(f\"Saved PMI figure to: {figure_path}\")\n",
    "except NameError:\n",
    "    print(\"Warning: 'fig_pmi' not defined. Plot cell may not have been run.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving figure: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
